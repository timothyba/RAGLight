from __future__ import annotations
from typing import Any, List
from typing_extensions import override
import logging
import re

from ..vectorestore.vectorStore import VectorStore
from ..embeddings.embeddingsModel import EmbeddingsModel
from ..rag.rag import RAG
from ..llm.llm import LLM
from ..config.settings import Settings


class RAT(RAG):
    """
    Retrieval-Augmented Thinking (RAT) class that extends the Retrieval-Augmented Generation (RAG) pipeline
    to incorporate a reasoning step using a dedicated reasoning LLM.

    This class performs an iterative reflection process to refine the input query based on retrieved context
    and a reasoning LLM, before generating a final answer.

    Attributes:
        reasoning_llm (LLM): The LLM used for reasoning and generating reflections.
        reflection (int): The number of reasoning iterations to perform for the input query.
    """

    def __init__(
        self,
        embedding_model: EmbeddingsModel,
        vector_store: VectorStore,
        reasoning_llm: LLM,
        llm: LLM,
        reflection: int = 1,
    ) -> None:
        """
        Initializes the RAT pipeline with the required components.

        Args:
            embedding_model (EmbeddingsModel): The embeddings model for vectorization.
            vector_store (VectorStore): The vector store for document retrieval.
            reasoning_llm (LLM): The LLM used for reasoning and iterative reflection.
            llm (LLM): The LLM used for generating the final answer.
            reflection (int, optional): The number of reasoning iterations to perform. Defaults to 1.
        """
        super().__init__(embedding_model, vector_store, llm)
        self.reasoning_llm: LLM = reasoning_llm
        self.reflection: int = reflection

    def set_reflection(self, reflection: int) -> None:
        """
        Set the reflection attribute for this instance.

        :param reflection: The new reflection value (integer) to assign.
        :type reflection: int
        :return: None
        """
        self.reflection = reflection

    def think(self, input: str) -> str:
        """
        Generates iterative reasoning or reflection based on the input question and retrieved documents.

        This method performs multiple reasoning iterations using the reasoning LLM. Each iteration
        refines the reflection based on the retrieved context and the reasoning generated by the LLM.

        Args:
            input (str): The initial question or query provided by the user.

        Returns:
            str: A final reflection or reasoning generated by the reasoning LLM.
                 If no reasoning is found, returns the last valid reflection or a default message.
        """
        reflection = input
        for _ in range(self.reflection):
            retrieved_docs = self.retrieve({"question": reflection})
            docs_content = "\n\n".join(
                doc.page_content for doc in retrieved_docs["context"]
            )
            prompt_json = {
                "question": input,
                "context": docs_content,
                "reflection": reflection,
            }
            response = self.reasoning_llm.generate(prompt_json)
            think = re.findall(Settings.THINKING_PATTERN, response, re.DOTALL)
            if not think:
                logging.warning("No reasoning found in the LLM response.")
                return reflection or "No reasoning available."
            reflection = f"Reflection about the problem: {think[0]}"
        return f"Reflection about the problem: {reflection}"

    @override
    def question_graph(self, question: str) -> str:
        """
        Executes the RAT pipeline for a given question by first generating a reflection and
        then invoking the state graph to generate a final answer.

        The reflection step enhances the reasoning process by iteratively refining the context
        and query using the `think` method before the final answer is generated.

        Args:
            question (str): The input question or query.

        Returns:
            str: The generated answer from the pipeline, which incorporates both
                 the initial question and the reasoning generated by the `think` method.
        """
        state = {"question": question, "reflection": self.think(question)}
        response = self.graph.invoke(state)
        return response["answer"]
